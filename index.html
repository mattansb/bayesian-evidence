<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Evaluating Evidence and Making Decisions using Bayesian Statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Mattan S. Ben-Shachar" />
    <meta name="date" content="2021-02-23" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






background-image: url(img/bg_main.png)
class: left, bottom, title-slide

# Evaluating Evidence and Making Decisions using Bayesian Statistics

&lt;h2&gt;ISCoP Conference 2021&lt;/h2&gt;

&lt;h3&gt;Mattan S. Ben-Shachar&lt;/h3&gt;

.right[
.big[&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt; [.white[tinyurl.com/ISCoP-2021-bayes]](https://tinyurl.com/ISCoP-2021-bayes)]  
.big[Presented at February 23, 2021]  
.small[(updated February 23, 2021)]
]

---












class: title-slide, center

# About Me

&lt;img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/&gt;

&lt;h2&gt;Mattan S. Ben-Shachar&lt;/h2&gt;

&lt;h3&gt;PhD Student + Stats Lover + R Developer&lt;/h3&gt;

.fade[Ben-Gurion University of the Negev&lt;br&gt;Beer Sheva, Israel]

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/&gt;&lt;/svg&gt; [.white[@mattansb]](https://twitter.com/mattansb) | &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; [.white[@mattansb]](https://github.com/mattansb)










---

background-color: var(--myred)
class: inverse

.center[
# About You
]

--

.big[
&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/&gt;&lt;/svg&gt; You use statistical models (in `R`)
- ANOVAs, regression
- Maybe some mixed models
]

--

.big[
&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/&gt;&lt;/svg&gt; You've heard about (and maybe even used) Bayes factors
]

--

.big[
&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M512 199.652c0 23.625-20.65 43.826-44.8 43.826h-99.851c16.34 17.048 18.346 49.766-6.299 70.944 14.288 22.829 2.147 53.017-16.45 62.315C353.574 425.878 322.654 448 272 448c-2.746 0-13.276-.203-16-.195-61.971.168-76.894-31.065-123.731-38.315C120.596 407.683 112 397.599 112 385.786V214.261l.002-.001c.011-18.366 10.607-35.889 28.464-43.845 28.886-12.994 95.413-49.038 107.534-77.323 7.797-18.194 21.384-29.084 40-29.092 34.222-.014 57.752 35.098 44.119 66.908-3.583 8.359-8.312 16.67-14.153 24.918H467.2c23.45 0 44.8 20.543 44.8 43.826zM96 200v192c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V200c0-13.255 10.745-24 24-24h48c13.255 0 24 10.745 24 24zM68 368c0-11.046-8.954-20-20-20s-20 8.954-20 20 8.954 20 20 20 20-8.954 20-20z"/&gt;&lt;/svg&gt; **You want to know *more* about Bayesian stats**
]

---















class: middle, big

&lt;svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 576 512"&gt;&lt;path d="M528 0H48C21.5 0 0 21.5 0 48v320c0 26.5 21.5 48 48 48h192l-16 48h-72c-13.3 0-24 10.7-24 24s10.7 24 24 24h272c13.3 0 24-10.7 24-24s-10.7-24-24-24h-72l-16-48h192c26.5 0 48-21.5 48-48V48c0-26.5-21.5-48-48-48zm-16 352H64V64h448v288z"/&gt;&lt;/svg&gt; Link to this presentation:  
[tinyurl.com/ISCoP-2021-bayes](https://tinyurl.com/ISCoP-2021-bayes)

&lt;br&gt;

&lt;svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; All the code and materials used in this workshop can be found on GitHub:  
[github.com/mattansb/bayesian-evidence-iscop-2021](https://github.com/mattansb/bayesian-evidence-iscop-2021)

---

















# Outline

--

- What is a Bayesian model?

--

- How to Bayes, even?

--

- Why to Bayes? (aka "Why is this better than how I currently model?")

--

- Demo: Building a Bayesian model
  - Posterior Estimates
  
--

  - **Evaluating Evidence and Making Decisions using Bayesian Statistics**

--

*Let us begin...*

---









class: title-slide, center, middle

&lt;h1&gt;It's all About the &lt;br&gt; &lt;s&gt;Bass&lt;/s&gt; Bayesian Modeling&lt;/h1&gt;

---









class: inverse

# What *is* a Bayesian model?

A Bayesian model is a statistical model where you use **probability** to represent **all uncertainty** within the model, both the uncertainty regarding the output but also the uncertainty regarding the input (aka parameters) to the model&lt;sup&gt;1&lt;/sup&gt;...

.footnote[
[1] B√•√•th (2015). *From [stackexchange](https://stats.stackexchange.com/a/129712/293056)*
]

???

- "uncertainty regarding the output" = how un/certain we are about our predictions.
- "uncertainty regarding the input" = how un/certain we are about our parameters.

--

... where probability expresses *a degree of belief* in an event.

---






class: title-slide, center, middle

# How to Bayes? &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 576 512"&gt;&lt;path d="M208 0c-29.9 0-54.7 20.5-61.8 48.2-.8 0-1.4-.2-2.2-.2-35.3 0-64 28.7-64 64 0 4.8.6 9.5 1.7 14C52.5 138 32 166.6 32 200c0 12.6 3.2 24.3 8.3 34.9C16.3 248.7 0 274.3 0 304c0 33.3 20.4 61.9 49.4 73.9-.9 4.6-1.4 9.3-1.4 14.1 0 39.8 32.2 72 72 72 4.1 0 8.1-.5 12-1.2 9.6 28.5 36.2 49.2 68 49.2 39.8 0 72-32.2 72-72V64c0-35.3-28.7-64-64-64zm368 304c0-29.7-16.3-55.3-40.3-69.1 5.2-10.6 8.3-22.3 8.3-34.9 0-33.4-20.5-62-49.7-74 1-4.5 1.7-9.2 1.7-14 0-35.3-28.7-64-64-64-.8 0-1.5.2-2.2.2C422.7 20.5 397.9 0 368 0c-35.3 0-64 28.6-64 64v376c0 39.8 32.2 72 72 72 31.8 0 58.4-20.7 68-49.2 3.9.7 7.9 1.2 12 1.2 39.8 0 72-32.2 72-72 0-4.8-.5-9.5-1.4-14.1 29-12 49.4-40.6 49.4-73.9z"/&gt;&lt;/svg&gt;

.bottom[To fit a Bayesian model you need...]

---

### .blue[A Prior]

A probability distribution representing your prior *belief* about the probability of possible values each parameter can take.

--

&gt; *"Sounds too subjective to be used in Science!"*  
&gt;     .center[\- You (2021)?]

--

In real life applications, you would be hard-pressed to just use whatever prior you like - you would need to somehow **justify your prior** (which requires domain specific knowledge).

.footnote[

Watch also [B√ºrkner (2018). *Why not to be afraid of priors (too much)*](https://www.youtube.com/watch?v=Uz9r8eV2erQ)

]

--

Similar to how you must also justify and use a reasonable likelihood function.

---









### .orange[A Likelihood Function]

What process best describes the (conditional) data generation process? 

--

For example:
- A .orange[binomial] likelihood function for **binary** data
- A .orange[Poisson] likelihood function for **count** data
- A .orange[cumulative multinomial] likelihood function for **ordinal** data
- An .orange[inverse Gaussian / ex-Gaussian / [other]] likelihood function for **reaction times**
- ...
- A .orange[Gaussian] likelihood function for **conditionally normal** data

The likelihood function tells us the *probability of observing our data given the value(s) of some parameter(s)*.

--

This function is used to ***update the priors***, resulting in ***The Posterior***...

---









### Prior + Likelihhod = .green[Posterior]

This is that whole pesky *Bayes' Rule* thing everyone keeps going on about:

.content-box-green[
`$$\overbrace{P(\theta|Data)}^{\text{Posterior}} = \frac{\overbrace{P(Data|\theta)}^{\text{Likelihood}} \times \overbrace{P(\theta)}^{\text{Prior}}}{P(Data)}$$`

In words:

The **posterior probability** of some parameter `\(\theta\)` having a value of `\(x\)`, is a equal to probability of the observed data occurring if that were the value of `\(\theta\)` (**the likelihood**), normalized by our **prior belief** that `\(\theta\)` can have a value of `\(x\)`.
]

.footnote[
We usually can only estimate the posterior distribution by sampling from it.
]

---











&lt;!-- Normal Priors --&gt;

--

![](index_files/figure-html/normal_prior1a-1.png)&lt;!-- --&gt;

---

![](index_files/figure-html/normal_prior1b-1.png)&lt;!-- --&gt;

---

![](index_files/figure-html/normal_prior1c-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior2-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior3-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/normal_prior4-1.png)&lt;!-- --&gt;

---









&lt;!-- Weird Priors --&gt;

???

You will usually see some bell-curve-like prior, but priors can, in theory, take any weird shape you can think of.

--

![](index_files/figure-html/weird_prior1-1.png)&lt;!-- --&gt;

???

This is also called a horse-shoe prior - used for regularization.

--

![](index_files/figure-html/weird_prior2-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/weird_prior3-1.png)&lt;!-- --&gt;

--

![](index_files/figure-html/weird_prior4-1.png)&lt;!-- --&gt;

???

The posterior is not only affected by how strong or weak or priors are, but also by how stong/weak or data is...

---






















&lt;!-- Just likelihood --&gt;

![](index_files/figure-html/lik_plot1-1.png)&lt;!-- --&gt;

???

Stronger data = larger sample sizes and stronger effects - lead to more specific likelihood functions.

--

![](index_files/figure-html/lik_plot2-1.png)&lt;!-- --&gt;![](index_files/figure-html/lik_plot2-2.png)&lt;!-- --&gt;

---











background-color: var(--myred)
class: inverse

# Why to Bayes?

&lt;h3&gt;&lt;i&gt;AKA&lt;/i&gt; "Why is this better than what I currently do?"&lt;/h3&gt;

&lt;hr&gt;

--

- **Speak in the language of probabilities** (*probabilitese?*).

&gt; *There is a 0.2 (posterior) probability of the treatment alleviating more than 3 ADHD symptoms.*

&gt; *There is a 0.85 (posterior) probability of realibility of the test being at least `\(\alpha &gt; 0.8\)`.*

--

- **The power of Priors**

  - Utilize prior knowledge - *add* the information gained from the current data to the existing corpus of knowledge. 

      - Not every study is *tabula rasa*.

  - Use priors to prevent over-fitting (regularization via horseshoe, spike-and-slab).

???

We don't have to invent the wheel... And we can use priors to be cautious (regularization)...

---















class: inverse

&lt;h1&gt;Why to Bayes?&lt;/h1&gt;

&lt;h3&gt;&lt;i&gt;AKA&lt;/i&gt; "Why is this better than what I currently do?"&lt;/h3&gt;

&lt;hr&gt;

**Fit complex models / to complex data**:

- Limiting the search space of our model's parameters to what is *a-priori* reasonable, reduces issues that plague other estimation methods.

  - failed convergence, local maxima, complete separation...
  
--

- With a likelihood function and a prior, you can add endless complexity to your model (even allow `\(n&lt;p\)`).
  
  - Easily model heteroscedasticity,
  - Model individual differences in ICC in HLM,
  - Easily obtain CIs for random effects,
  - ...

--

- **Some types of models cannot practically be analyzed using frequentists methods** ü§∑ [(Rouder &amp; Lu, 2005)](https://twitter.com/Nate__Haines/status/1360227275711668228)

---








class: title-slide, bottom

# Demo

&lt;h2&gt; Let's get our hands dirty...&lt;/h2&gt;

.right[
See the full analysis script [here &lt;svg style="height:0.8em;top:.04em;position:relative;fill:#bf94e4;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt;](files/full%20analysis%20script.nb.html)
]

???

Due to time constraints (fitting Bayesian models does take some time), I will walk you through the process of model fitting, exploration, and inference.

---

class: middle

We will be looking at a regression model, 

but the tools from this demo can be applied to Bayesian [SEM](https://faculty.missouri.edu/~merklee/blavaan/), IRT, SDT, [etc](https://cran.r-project.org/view=Bayesian)...

---










class: small

## The Data

--

Thirty 4 year old children completed **Flanker's task**. .small[(real data.)]

.pull-left[
.center[
**Congruent**

&lt;img src="img/flanker_fishC.png" width="80%"/&gt;
]
]

.pull-right[
.center[
**Incongruent**

&lt;img src="img/flanker_fishI.png" width="80%"/&gt;
]
]

.center[
**Neutral**

&lt;img src="img/flanker_fishN.png" width="40%"/&gt;
]

.footnote[
*Image from [pixy](https://pixy.org/4293048/).*
]

--

We will be examining their **Interference** (Incongruent - Neutral) and **Facilitation** (Neutral - Congruent) effects, **controlling for age** (in months).

---










We will be working in **`R`** with the following packages:

- `brms` for Bayesian Regression Models with *Stan*.

  - *Stan* is a probabilistic programming language

--

- `emmeans` for extracting estimates / contrasts / slopes from the model.

- `bayestestR` for descriptive and inferential statistics.

--

- Plots are made with `ggplot2` + `patchwork` + `tidybayes` + `ggdist` + `see`.



.footnote[
[See other package versions and packages used &lt;svg style="height:0.8em;top:.04em;position:relative;fill:#bf94e4;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt;](files/full%20analysis%20script.nb.html#setup)
]

---











class: small

## Building a Bayesian Model


```r
m_flanker &lt;- brm(
* RT ~ Congruency + age_mo + (Congruency | id),
* data = child_flanker,
  prior = 
    # Two parameters for Congruency 
    set_prior("student_t(3, 0, 100)", class = "b",
              coef = c("Congruency1", "Congruency2")) +
    # Slope of age_mo
    set_prior("student_t(3, 0, 1000)", class = "b",
              coef = "age_mo"),
  family = gaussian())
```

We will be fitting an hierarchical linear model - predicting (single trial) RTs from `Congruency` (I, N, C) which is nested within each child (`id`) - controlling for the children's age (in months, `age_mo`).

This is essentially a repeated measures ANCOVA.

--

.content-box-red[
Note: For `Congruency` I've used ***orthonormal* dummy-coding**. This is important, but ‚è≥! Read more about that [here](https://easystats.github.io/bayestestR/articles/bayes_factors.html#contr_bayes).
]

---










class: small

&lt;h2&gt;Building a Bayesian Model&lt;/h2&gt;


```r
m_flanker &lt;- brm(
  RT ~ Congruency + age_mo + (Congruency | id), 
  data = child_flanker, 
* prior =
*   # Two parameters for Congruency
*   set_prior("student_t(3, 0, 100)", class = "b",
*             coef = c("Congruency1", "Congruency2")) +
*   # Slope of age_mo
*   set_prior("student_t(3, 0, 1000)", class = "b",
*             coef = "age_mo"),
  family = gaussian())
```

For our fixed effects, we will be somewhat conservative and use a scaled *t*(3)-prior centered on 0. This prior has the benefit of the scaling factor giving the range where 60% of the prior's mass is.

--

- In adults, the Flanker effect is about 20-50ms. Here we have 4yo - reasonable (?) that any differences between means (effect) would be **~100ms**, which we will use as our scaling factor ([Jonkman et al, 1999](https://doi.org/10.1111/1469-8986.3640419)).

--

- Prior on effect of age - no idea. We will use a weakly informative prior scaled to 1000ms/month (covering a very large range of possible effects).

---

class: small

&lt;h2&gt;Building a Bayesian Model&lt;/h2&gt;


```r
m_flanker &lt;- brm(
  RT ~ Congruency + age_mo + (Congruency | id), 
  data = child_flanker, 
* prior =
*   # Two parameters for Congruency
*   set_prior("student_t(3, 0, 100)", class = "b",
*             coef = c("Congruency1", "Congruency2")) +
*   # Slope of age_mo
*   set_prior("student_t(3, 0, 1000)", class = "b",
*             coef = "age_mo"),
  family = gaussian())
```

.content-box-red[
Notes:
  - By default, `brms` sets **flat** (*diffused, extremely uninformative*) priors for fixed effects.
  - We can also set a prior of `sigma` (error variance), and many others. See more options with `brms::get_prior()`.
]

---











class: small

&lt;h2&gt;Building a Bayesian Model&lt;/h2&gt;


```r
m_flanker &lt;- brm(
  RT ~ Congruency + age_mo + (Congruency | id), 
  data = child_flanker, 
  prior = 
    # Two parameters for Congruency 
    set_prior("student_t(3, 0, 100)", class = "b",
              coef = c("Congruency1", "Congruency2")) + 
    # Slope of age_mo 
    set_prior("student_t(3, 0, 1000)", class = "b",
              coef = "age_mo"), 
* family = gaussian())
```

We will be using a Gaussian likelihood function of `\(RT \sim N(\mu_i, \sigma^2)\)`, where `\(\mu_i =a + \sum b_j X_{ij}\)`.

AKA, a boring linear regression.

---











### Prior &amp; Posterior Checks

???

These checks just make sure that our model is very generally reasonable, and that we've done a good job of sampling from the posterior...

--

&lt;img src="img/footagenotfound.jpg" width="80%" /&gt;

.footnote[
But you can find them, and more, in [the full analysis script &lt;svg style="height:0.8em;top:.04em;position:relative;fill:#bf94e4;" viewBox="0 0 512 512"&gt;&lt;path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/&gt;&lt;/svg&gt;](files/full%20analysis%20script.nb.html)...
]

---











## Explore the Model




.pull-left[
Let's look at the posteriors of the estimated means for the Congruency conditions:


```r
means_Congruency &lt;-
  emmeans(m_flanker, ~ Congruency)
```

&lt;hr&gt;

]

---

&lt;h2&gt;Explore the Model&lt;/h2&gt;

.pull-left[
Let's look at the posteriors of the estimated means for the Congruency conditions:


```r
means_Congruency &lt;-
  emmeans(m_flanker, ~ Congruency)
```

&lt;hr&gt;

Frequentist estimation methods (such as **OLS** or **maximum likelihood (ML)**) produce a point estimate for each parameter.

But in Bayes we have not a single value, but .green[a whole distribution of values]!

We we can either .green[present the whole distribution, *as is*]...

]


--

.pull-right[

![](index_files/figure-html/plot_cong_means-1.png)&lt;!-- --&gt;

]

---








Or we can summarize the posterior distribution:

--

.pull-left[

.purple[A Representative Value]

.small[(in lieu of a point estimate)]

- Median (most common)
- Mean
- Maximum A Posteriori (MAP)

]

--

.pull-right[
.red[Credible Intervals (CIs)]


- The Highest Density Interval (HDI; most common)
- The Equal-Tailed Interval (ETI)

]

--

&lt;hr&gt;


```r
describe_posterior(means_Congruency, 
                   centrality = "median",
                   ci = 0.89, ci_method = "hdi",
                   test = NULL)
```

```
## # Description of Posterior Distributions
## 
## Parameter   |   Median |               89% CI
## ---------------------------------------------
## Incongruent | 1602.487 | [1420.115, 1753.119]
## Neutral     | 1433.930 | [1286.052, 1587.072]
## Congruent   | 1490.332 | [1341.278, 1639.822]
```

---











background-color: var(--myred)
class: inverse, center, middle, title-slide

## Evaluating Evidence and Making Decisions &lt;br&gt; using Bayesian Statistics
&lt;h2&gt;&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/&gt;&lt;/svg&gt;&lt;/h2&gt;

---











We are limiting our discussion to evaluating evidence for **single estimates / parameters** (expected values, slopes, contrasts...).

But it is also possible to evaluating evidence for multiple parameters, with order restrictions and model comparisons. (Maybe next year...)

--

&lt;hr&gt;

We will be looking at two contrasts: the Interference and Facilitation effects:


```r
diffs_Congruency &lt;- contrast(means_Congruency, 
                             list(Interference = c(1, -1, 0),
                                  Facilitation = c(0,  1, -1)))

describe_posterior(diffs_Congruency, test = NULL)
```

```
## # Description of Posterior Distributions
## 
## Parameter    |  Median |              89% CI
## --------------------------------------------
## Interference | 166.484 | [  48.509, 285.054]
## Facilitation | -56.012 | [-154.245,  35.098]
```


---











class: small

### The Probability of Direction

- The maximal probability of the estimate being strictly directional (larger or smaller than 0).

- Generally ranges from 50% (no preference) to 100%.

--

.pull-left[


```r
p_direction(diffs_Congruency)
```

```
## # Probability of Direction (pd)
## 
## Parameter    |     pd
## ---------------------
## Interference | 98.58%
## Facilitation | 83.58%
```

]

.pull-right[

![](index_files/figure-html/pd_plot-1.png)&lt;!-- --&gt;

]

--

For the Interference effect it seems like these is a high probability of direction, but not that great for the Facilitation effect ( `\(p_d\)` &lt; 0.95 ). 

--

- &lt;b&gt;.green[Pros]&lt;/b&gt;: Easy to understand; Resembles the *p*-value - `\(r \simeq -1\)`. &lt;sup&gt;*&lt;/sup&gt;

- &lt;b&gt;.red[Cons]&lt;/b&gt;: like the *p*-values, a *low* `\(p_d\)` cannot be used to support the null.

---















class: small

### *p*-MAP


- The *density ratio* between the null and the MAP value.

- Values range from 1 (the null *is* the MAP) to ~0 (the MAP is much much more probable than the null).

--

.pull-left[


```r
p_map(diffs_Congruency)
```

```
## # MAP-based p-value
## 
## Parameter    | p_MAP
## --------------------
## Interference | 0.086
## Facilitation | 0.651
```

]

.pull-right[

![](index_files/figure-html/pmap_plot-1.png)&lt;!-- --&gt;

]

--

For the Interference effect it seems like the MAP is more th 10 times more probable than the null. But for the Facilitation effect it is not even twice as probable. 

--

- &lt;b&gt;.green[Pros]&lt;/b&gt;: Closely related to LRT tests - familiar; Also closely associated with the *p*-value.

- &lt;b&gt;.red[Cons]&lt;/b&gt;: Again, a *high* *p*-MAP cannot be used to support the null.

---














### *p*-ROPE

- The probability that our estimate is *basically* null.

--

- We first define a **Region of Practical Equivalence (ROPE)** - a range of effects that are, for any practical purposes, the same as no effect at all. 

--

For the Congruency effects, we will define any effect that is smaller in magnitude than 30ms, to be consider to be just as good as no effect at all - so ROPE [-30, +30].

--

.small[We can also have a one sided ROPE, with [-Inf, +30], etc.]

---

class: small

&lt;h3&gt;&lt;i&gt;p&lt;/i&gt;-ROPE&lt;/h3&gt;

- How much of the posterior falls in the ROPE.

  - Or: How much of the most probable values (e.g., those in the HDI) fall in the ROPE.

--

.pull-left[


```r
rope(diffs_Congruency, 
     range = c(-30, 30), ci = 0.89)
```

```
## # Proportion of samples inside the ROPE [-30.00, 30.00]:
## 
## Parameter    | inside ROPE
## --------------------------
## Interference |      0.00 %
## Facilitation |     29.99 %
```

]

.pull-right[

![](index_files/figure-html/therope_plot-1.png)&lt;!-- --&gt;

]

--

It is very improbable that the Interference effect is very small. *But* there is about a 30% that among 4 year olds, there is no Facilitation effect - ([though not very conclusive](https://easystats.github.io/bayestestR/articles/guidelines.html#significance)) we are supporting the null!

???

If you've ever heard that "Bayes is good for small samples" this is what is meant by that: that unlike frequentist methods where small samples and non-significant results leave you high and dry, Bayes allows you to same *something*, weak as it may be.

---














The *p*&lt;sub&gt;*d*&lt;/sub&gt;, *p*-MAP and ROPE are **posterior based methods** - they inform us about the accumulated information in the priors + our data.

--

Often we are interested in **what has been *learned* in the current study, from the current data**.

--

.pull-left[

E.g., by now it's clear that there exists an Interference effect in Flanker's task. But **which values of the effect are supported or contradicted by the *current* data?** Maybe our data supports the null value(s) - what can be learnt from that?

To answer these types of questions we can *compare* .blue[The Prior] to .green[The Posterior] to see .orange[what our data taught us] - what values became more / less plausible.

]

--


.pull-right[

![](index_files/figure-html/support_plot-1.png)&lt;!-- --&gt;

]

--

We can use this information to look at different *sets of parameter values* - or **hypotheses** - E.g. `\(H_{small}: \theta \in [-3, 3]\)`, `\(H_{positive}: \theta \in [0, \infty]\)` and ask:

&gt; Which *hypothesis* is supported *more* by the data?

---


















### The Bayes Factor

This index of evidence is a ***Bayes Factor***:

- It quantifies how the prior was *updated* to the posterior.

- It compares two "hypotheses".

--

**Any** measure that quantifies this üëÜ is a Bayes factor.

.content-box-red[

There are many different type of questions that can be answered with Bayes factors - we will be looking at two.

]

???

You maybe have used Bayes factors to compare between models... Those too have these properties.

--

&lt;hr&gt;

For technical reasons we need a model that represents *only our priors* - which we will then *compare* to the results from our updated (posterior) model.

We can do that with the `unupdate()` function:


```r
# Get the priors only ("un-update" the model).
m_flanker_prior &lt;- unupdate(m_flanker)
```

---









### The Null-Interval Bayes Factor

The null-interval Bayes factor is an extension of the ROPE test;

--

&gt; How has the *relative probability*&lt;sup&gt;[1]&lt;/sup&gt; of the the effect being practically null changed? Does the data support or contradict the effect being null?

.footnote[
[1] The odds of the effect being inside the ROPE to it being outside the ROPE.
]

--

&lt;hr&gt;

The two hypotheses we will be comparing, using the same ROPE:

- `\(H_0: \text{effect} \in [-30, +30]\)`  
- `\(H_A: \text{effect} \notin [-30, +30]\)`  
  - Or: `\(H_A: \text{effect} &lt; -30\)` or `\(+30 &lt; \text{effect}\)`
















---

class: small

&lt;h3&gt;The Null-Interval Bayes Factor&lt;/h3&gt;

--

.pull-left[


```r
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = c(-30, 30) # same ROPE as before
)
```

```
## # Bayes Factor (Null-Interval)
## 
## Parameter    |    BF
## --------------------
## Interference | 6.022
## Facilitation | 0.518
## 
## * Evidence Against The Null: [-30, 30]
```

]

--

.pull-right[

![](index_files/figure-html/bf_ROPE_plot-1.png)&lt;!-- --&gt;

]

--

- For the Interference effect, the ROPE has *become* relatively less probable - with the data giving 6 times more support for non-ROPE values.

--

- For the Facilitation effect, the ROPE has *become* relatively **more** probable - with the data giving (1/0.5 =) 2 times more support compared to the non-ROPE values.

---











### The Point-Null Bayes Factor

The point-null can be thought of as the null-interval Bayes factor with an infinitesimally small ROPE - that includes only one null value, exactly.

--

&gt; How has the probability&lt;sup&gt;[1,2]&lt;/sup&gt; of the the null value changed? Does the data support or contradict the effect being null?

This Bayes factor is also called the *Savage-Dickey density ratio*, .small[and it is analogous to a Bayes factor comparing two nested models.]

.footnote[

[1] Actually the density of the null.  
[2] This is also relative - if the null became more probable, necessarily the non-null values became less, and vice versa.

]

--

&lt;hr&gt;

The two hypotheses we will be comparing:

- `\(H_0: \text{effect} = 0\)`  
- `\(H_A: \text{effect} \neq 0\)`  
  - Or: `\(H_A: \text{effect} &lt; 0\)` or `\(0 &lt; \text{effect}\)`

---















class: small

&lt;h3&gt;The Point-Null Bayes Factor&lt;/h3&gt;

--

.pull-left[


```r
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = 0
)
```

```
## # Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter    |    BF
## --------------------
## Interference | 5.930
## Facilitation | 0.591
## 
## * Evidence Against The Null: [0]
```

]

--

.pull-right[

![](index_files/figure-html/bf_point_plot-1.png)&lt;!-- --&gt;

]

--

- For the Interference effect, the *mass* of the posterior is shifted *away* from the null (compared to the prior) - the data giving ~6 times more support for non-null values.

--

- For the Facilitation effect the mass has moved *towards* 0, the data giving (1/0.6 =) 1.7 times more support compared to the non-null values.

---

class: small

&lt;h3&gt;The Point-Null Bayes Factor&lt;/h3&gt;

.pull-left[


```r
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = 0
)
```

```
## # Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter    |    BF
## --------------------
## Interference | 5.930
## Facilitation | 0.591
## 
## * Evidence Against The Null: [0]
```

]

.pull-right[


```r
bayesfactor_parameters(
  diffs_Congruency, 
  prior = m_flanker_prior,
  null = c(-30, 30) # same ROPE as before
)
```

```
## # Bayes Factor (Null-Interval)
## 
## Parameter    |    BF
## --------------------
## Interference | 6.022
## Facilitation | 0.518
## 
## * Evidence Against The Null: [-30, 30]
```

]

.content-box-green[

Here the point-null and the null-interval BFs gave similar results, but that need not be the case - depending on the effect size, the definition of the ROPE, the sample size, etc.

]

---

















---

### Other Bayes Factors

- **Directional** null-interval / point-null Bayes factors

  - e.g., [-30, +30] *vs* [+30, Inf]

- Bayes factor for **dividing hypotheses**

  - e.g., [-Inf, 0] *vs* [0, Inf]

- **Model restricted** Bayes factors

  - [Incongruent &gt; Neutral &gt; Congruent] vs [Incongruent ‚â† Neutral ‚â† Congruent]
  
- And more...

Read more about these Bayes factors [here](https://easystats.github.io/bayestestR/articles/bayes_factors.html)!

---













## Age

--

.pull-left[

For *covariates*, we can present the posterior distribution of slopes, but be can also present a *trace plot* of slopes from the posterior.

.small[For example, we can sample 100 slopes from the posterior, and plot each one:]

]

--

.pull-right[

![](index_files/figure-html/age_lines_plot-1.png)&lt;!-- --&gt;

]

---

&lt;h2&gt;Age&lt;/h2&gt;

.pull-left[

Here too we can summarize the posterior distribution:


```r
slope_age &lt;- emtrends(m_flanker, ~1, "age_mo")

describe_posterior(slope_age, test = NULL)
```

```
## # Description of Posterior Distributions
## 
## Parameter | Median |             89% CI
## ---------------------------------------
## overall   | 22.476 | [-47.475, 101.115]
```

]

--

.pull-right[

![](index_files/figure-html/slopes_standard_plot-1.png)&lt;!-- --&gt;

]

---
















#### *p*-Direction &amp; *p*-MAP

--

.pull-left[


```r
p_direction(slope_age)
```

```
## # Probability of Direction (pd)
## 
## Parameter |     pd
## ------------------
## overall   | 67.70%
```

]


--


.pull-right[


```r
p_map(slope_age)
```

```
## # MAP-based p-value
## 
## Parameter | p_MAP
## -----------------
## overall   | 0.830
```

]

--

&lt;br&gt;&lt;br&gt;

Not very decisive‚Ä¶ (remember, these cannot be used to support the null!)

---














#### *p*-ROPE

For the ROPE - I think any effect smaller an overall change of less than 500ms a year = ~40ms a month, is practically 0 (you may disagree‚Ä¶):

--


```r
rope(slope_age, range = c(-40, 40), ci = 0.89)
```

```
## # Proportion of samples inside the ROPE [-40.00, 40.00]:
## 
## Parameter | inside ROPE
## -----------------------
## overall   |     63.89 %
```

--

There is about a 60% probability that the effect of age on reaction times is *practically* nothing!

Not strongly conclusive, but at the very least it is suggestive!

---














class: small

#### Bayes Factor

--


```r
bayesfactor_parameters(
  slope_age,
  prior = m_flanker_prior,
  null = 0
)
```

```
## # Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter |    BF
## -----------------
## overall   | 0.052
## 
## * Evidence Against The Null: [0]
```

Wow! It seems that the data strongly support (by a factor of 1/0.05 = 20) the effect of age being null over it being non-null!

--

*But wait* - the Bayes factor measures the change from the prior to the posterior‚Ä¶ But what was our prior here?

---

class: small

&lt;h4&gt;Bayes Factor&lt;/h4&gt;

.pull-left[


```r
bayesfactor_parameters(
  slope_age,
  prior = m_flanker_prior,
  null = 0
)
```

```
## # Bayes Factor (Savage-Dickey density ratio)
## 
## Parameter |    BF
## -----------------
## overall   | 0.052
## 
## * Evidence Against The Null: [0]
```

]

--

.pull-right[

![](index_files/figure-html/slopes_BF_plot-1.png)&lt;!-- --&gt;

]

--

We used a super vague prior - which give some non-trivial probability to extreme effects!

So is it really surprising that the posterior is now, relatively closer to the *null*? ***No.***

???

In our prior, the null was very very importable - it is not therefore surprising that it became *more* probable.

--

.content-box-red[
With wide and uninformative enough priors, the Bayes factor will **always favor the null / ROPE**!  
DO NOT COMPUTE BAYES FACTORS WITH UNINFORMATIVE PRIORS! &lt;sup&gt;*&lt;/sup&gt;
]

???

This is only an issue if one of your hypotheses is a point, or almost a point. 

---















background-color: var(--myred)
class: inverse

# Recommendations

&lt;h3&gt;&lt;i&gt;What to actually report?&lt;/i&gt;&lt;/h3&gt;

We (Makowski et al., 2019) [recommend](https://easystats.github.io/bayestestR/articles/guidelines.html) reporting for inferential statistics:

--

- **The *p*-direction**: Easy to understand, easy to "translate" to *p*-values.

--

- ***p*-ROPE**: Provides information about the practical relevance of the effect, and allows to accept the null.

--

*If* informed priors are used,

- **Bayes factor** .small[(instead or in addition to the *p*-ROPE)]: Provides information about hypotheses supported or contradicted by the data.

---














class: title-slide

# Summary

&lt;h3&gt;&lt;i&gt;What you now know! &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 640 512"&gt;&lt;path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/&gt;&lt;/svg&gt;&lt;/i&gt;&lt;/h3&gt;

- What a Bayesian model *is*.

- What Bayes can give you, that no one else can.

- A taste of Bayesian model fitting with `brms`.

- The richness of inferences that can be made with Bayesian statistics.

---










class: title-slide, small

# Suggested Reading

&lt;h4&gt;For Bayesian Beginners&lt;/h4&gt;

- Makowski, D., Ben-Shachar, M. S., Chen, S. H., &amp; L√ºdecke, D. (2019). [Indices of effect existence and significance in the Bayesian framework. *Frontiers in psychology, 10*, 2767.](https://doi.org/10.3389/fpsyg.2019.02767)

  - [`bayestestR` guides and articles.](https://easystats.github.io/bayestestR)

- Van de Schoot, R. et al (2021). [Bayesian statistics and modelling. *Nature Reviews Methods Primers, 1*(1), 1-26.](https://doi.org/10.1038/s43586-020-00001-2)

- [Bayesian Inference for Psychology. *Psychonomic Bulletin and Review*.](https://scholar.google.co.il/scholar?q=Bayesian+inference+for+psychology+Psychonomic+Bulletin+and+Review)

#### Books

- Kruschke, J. (2014). Doing bayesian data analysis: A tutorial with r, jags, and stan. Academic Press.

- McElreath, R. (2018). Statistical rethinking: A bayesian course with examples in r and stan. Chapman; Hall/CRC.

  - [Richard's YouTube channel](https://www.youtube.com/channel/UCNJK6_DZvcMqNSzQdEkzvzA)

---








background-color: var(--myred)
class: inverse

.pull-left[
&lt;img style="border-radius: 50%;" src="https://mattansb.github.io/CV/headshots/BrainOrange.jpg" width="150px"/&gt;&lt;img src="img/BGU-logo-round-clear.png" width="20%" /&gt;&lt;img src="img/lab_logo.png" width="20%" /&gt;

# Thank you!

&lt;h3&gt;Follow me!&lt;/h3&gt;

&lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/&gt;&lt;/svg&gt; [.white[@mattansb]](https://twitter.com/mattansb) | &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 496 512"&gt;&lt;path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/&gt;&lt;/svg&gt; [.white[@mattansb]](https://github.com/mattansb) | &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 448 512"&gt;&lt;path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/&gt;&lt;/svg&gt; [.white[Blog]](https://shouldbewriting.netlify.com/)

]


.pull-right[
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
.center[
&lt;img src="img/easystats.png" width="30%" /&gt;&lt;img src="img/bayestestR.png" width="30%" /&gt;
]

.small[
The [**`bayestestR`**](https://easystats.github.io/bayestestR) package is part of the `easystats` project. Core team members:

- Me üëã

- Dominique Makowski ([@Dom_Makowski](https://twitter.com/Dom_Makowski))

- Daniel L√ºdecke ([@strengejacke](https://twitter.com/strengejacke))

- Indrajeet Patil ([@patilindrajeets](https://twitter.com/patilindrajeets))
]
]

.footnote[
*Slides created with the R package [**`xaringan`**](https://github.com/yihui/xaringan).*
]

---









background-image: url(img/boyfriend2.jpg)
class: right, bottom

[.black[@kareem_carr]](https://twitter.com/kareem_carr/status/1356986263975395333)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
